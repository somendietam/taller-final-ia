import streamlit as st
from PIL import Image
import easyocr
import os
import numpy as np
from groq import Groq
from huggingface_hub import InferenceClient

# --- CONFIGURACI칍N DE LA P츼GINA Y CLAVES ---

st.set_page_config(page_title="Taller IA: OCR + LLM", layout="wide")

# Cargar las claves de API desde los secretos de Streamlit
# Aseg칰rate de que los nombres coincidan con los que pusiste en Streamlit Cloud
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
    HUGGINGFACE_API_KEY = st.secrets["HUGGINGFACE_API_KEY"]
except KeyError:
    st.error("No se encontraron las claves de API en los secretos de Streamlit.")
    st.stop()

# --- M칍DULO 1: EL LECTOR DE IM츼GENES (OCR) [cite: 58] ---

# Desaf칤o de Cach칠: Usamos @st.cache_resource
# para cargar el modelo OCR solo una vez.
@st.cache_resource
def load_ocr_model():
    """Carga el modelo EasyOCR en memoria (cacheado)."""
    # Usamos 'es' (espa침ol) e 'en' (ingl칠s)
    reader = easyocr.Reader(['es', 'en'], gpu=False) 
    return reader

# 1. Crear la Interfaz B치sica [cite: 61]
st.title("Taller IA: Construcci칩n de una Aplicaci칩n Multimodal ")
st.header("M칩dulo 1: Lector de Im치genes (OCR) 游닞")

# 2. Implementar la Carga de Archivos [cite: 66]
uploaded_file = st.file_uploader(
    "Sube una imagen para extraer el texto",
    type=["png", "jpg", "jpeg"] # [cite: 68]
)

# 3. Cargar y Ejecutar el Modelo OCR
if uploaded_file is not None:
    # Mostrar la imagen subida [cite: 73]
    image = Image.open(uploaded_file)
    st.image(image, caption="Imagen subida", use_column_width=True)

    # Convertir la imagen para EasyOCR [cite: 74]
    # Necesitamos pasarla como bytes o como un array de numpy
    img_bytes = uploaded_file.getvalue()
    
    with st.spinner("Procesando imagen con OCR..."):
        # Cargar el modelo (lo tomar치 del cach칠 si ya est치 cargado)
        reader = load_ocr_model()
        
        # 4. Procesar y Mostrar Resultados [cite: 72]
        # Ejecutar el modelo OCR [cite: 75]
        results = reader.readtext(img_bytes)
        
        # Juntar todo el texto extra칤do
        extracted_text = " ".join([res[1] for res in results])

        # Desaf칤o de Persistencia:
        # Guardar el texto extra칤do en el st.session_state
        st.session_state['extracted_text'] = extracted_text
        
        # Mostrar el texto extra칤do [cite: 76]
        st.text_area(
            "Texto Extra칤do por OCR:",
            extracted_text,
            height=250,
            key="ocr_output"
        )

# --- M칍DULOS 2 y 3: CONEXI칍N CON LLMS Y FLEXIBILIDAD [cite: 77, 95] ---

# Solo mostramos esta secci칩n si ya hay texto extra칤do [cite: 88, 94]
if 'extracted_text' in st.session_state and st.session_state['extracted_text']:
    
    st.divider()
    st.header("M칩dulos 2 y 3: An치lisis con LLMs 游")
    
    # Texto extra칤do del estado de la sesi칩n
    text_to_analyze = st.session_state['extracted_text']

    # --- Interfaz de Usuario (UI) ---
    
    col1, col2 = st.columns(2)
    
    with col1:
        # M칩dulo 3: Elecci칩n de Proveedor [cite: 102]
        provider = st.radio(
            "Elige el proveedor de LLM:",
            ("GROQ", "Hugging Face"),
            key="provider"
        )

        # M칩dulo 2: Elecci칩n de Tarea [cite: 85]
        task_prompt = st.selectbox(
            "Elige la tarea a realizar:",
            (
                "Resumir el texto en 3 puntos clave",
                "Identificar las entidades principales (personas, lugares, organizaciones)",
                "Traducir el texto al ingl칠s",
                "Analizar el sentimiento del texto (positivo, negativo o neutral)",
                "Generar 3 preguntas sobre el texto"
            ),
            key="task"
        )
        
        # M칩dulo 2: Elecci칩n de Modelo (solo para GROQ) [cite: 84]
        if provider == "GROQ":
            model_selection = st.selectbox(
                "Elige el modelo de GROQ:",
                ("llama3-8b-8192", "mixtral-8x7b-32768", "gemma-7b-it"),
                key="groq_model"
            )
        else:
            # M칩dulo 3: Modelos de Hugging Face
            model_selection = st.text_input(
                "Modelo de Hugging Face (ej: mistralai/Mixtral-8x7B-Instruct-v0.1):",
                "mistralai/Mixtral-8x7B-Instruct-v0.1",
                key="hf_model"
            )

    with col2:
        # M칩dulo 3: Control de Par치metros [cite: 98]
        temperature = st.slider(
            "Temperatura (Creatividad) [cite: 99]",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            key="temperature"
        )
        
        max_tokens = st.slider(
            "M치ximos Tokens (Longitud) [cite: 99]",
            min_value=50,
            max_value=4096,
            value=512,
            step=64,
            key="max_tokens"
        )

    # M칩dulo 2: Bot칩n de An치lisis [cite: 86]
    analyze_button = st.button("Analizar Texto con LLM", type="primary")

    # --- L칩gica de la API ---
    
    if analyze_button:
        with st.spinner(f"Analizando texto con {provider}... Por favor espera."):
            try:
                # M칩dulo 3: L칩gica Condicional [cite: 103]
                if provider == "GROQ":
                    # M칩dulo 2: L칩gica de la API de GROQ [cite: 87]
                    client = Groq(api_key=GROQ_API_KEY) # [cite: 88]
                    
                    # Estructura correcta del prompt [cite: 90]
                    messages = [
                        {
                            "role": "system",
                            "content": f"Eres un asistente experto. El usuario te dar치 un texto y una tarea. Debes realizar la tarea solicitada sobre el texto. La tarea es: {task_prompt}."
                        },
                        {
                            "role": "user",
                            "content": f"El texto para analizar es el siguiente:\n\n---\n{text_to_analyze}\n---"
                        }
                    ]
                    
                    # Llamada a la API [cite: 89, 100]
                    chat_completion = client.chat.completions.create(
                        messages=messages,
                        model=model_selection,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    # Mostrar la respuesta [cite: 91]
                    response_content = chat_completion.choices[0].message.content
                    st.markdown("### Respuesta de GROQ")
                    st.markdown(response_content)

                elif provider == "Hugging Face":
                    # M칩dulo 3: L칩gica de la API de Hugging Face [cite: 104]
                    client = InferenceClient(token=HUGGINGFACE_API_KEY) # [cite: 106]
                    
                    # Estructura del prompt para un modelo instruct
                    # (Usamos text_generation que es m치s flexible [cite: 107])
                    prompt = f"""<s>[INST] Eres un asistente experto. El usuario te dar치 un texto y una tarea.
Tarea: {task_prompt}
Texto:
{text_to_analyze}
[/INST]
Respuesta:"""
                    
                    # Llamada a la API [cite: 107]
                    response = client.text_generation(
                        model=model_selection,
                        prompt=prompt,
                        max_new_tokens=max_tokens,
                        temperature=max(temperature, 0.01) # Temp 0 no es v치lida en HF, usamos 0.01
                    )
                    
                    # Mostrar la respuesta [cite: 110]
                    st.markdown("### Respuesta de Hugging Face")
                    st.markdown(response)

            except Exception as e:
                st.error(f"Error al contactar la API de {provider}: {e}")
